<p align="center">
  <img src="report/Accuracy.png" alt="Sign Language Detection" width="700"/>
</p>

# âœ‹ Sign Language to Text using CNN

A complete **Deep Learning project** that recognizes and translates **hand sign gestures** into text using **Convolutional Neural Networks (CNN)** and **MediaPipe hand tracking**.  
Built step-by-step from dataset preparation to model training, evaluation, and real-time webcam-based prediction.

---

## ðŸ§­ Project Structure

```text
Sign-Language-Detection/
â”‚
â”œâ”€â”€ dataset/                      # training images (6 gesture classes)
â”‚
â”œâ”€â”€ model/                        # trained CNN model (.h5 file)
â”‚   â””â”€â”€ sign_language_cnn_model_full.h5
â”‚
â”œâ”€â”€ notebooks/                    # Python scripts for training and prediction
â”‚   â”œâ”€â”€ train_cnn.py
â”‚   â””â”€â”€ realtime_predict.py
â”‚
â”œâ”€â”€ report/                       # accuracy and loss plots
â”‚   â””â”€â”€ training_curve.png
â”‚
â”œâ”€â”€ requirements.txt              # all dependencies (Python + libraries)
â”œâ”€â”€ venv/                         # virtual environment
â””â”€â”€ README.md
```

---

## ðŸŽ¯ Project Overview

This project demonstrates an **end-to-end computer vision workflow**:

1. **Dataset Creation** â€“ hand gesture images (Bye, Hello, No, Perfect, Thank You, Yes).  
2. **Data Augmentation** â€“ apply rotation, zoom, flip, brightness adjustments for diversity.  
3. **Model Training** â€“ train a CNN model with TensorFlow/Keras.  
4. **Evaluation** â€“ visualize accuracy and loss across epochs.  
5. **Real-Time Detection** â€“ integrate MediaPipe + OpenCV for live webcam prediction.  

---

## ðŸ§  Dataset

- **Classes:** 6 (Bye, Hello, No, Perfect, Thank You, Yes)  
- **Images per Class:** ~400  
- **Total Samples:** ~2400  
- **Split:** 80% training, 20% validation  
- Each gesture stored in its own subfolder under `dataset/`.

**Note:**
- For GitHub space optimization, only 5 sample images per class are stored in the dataset/ folder.
- The complete dataset (â‰ˆ2400 images) is used locally for model training.
---

## ðŸ§± Model Architecture

| Layer | Type | Filters | Activation |
|-------|------|----------|-------------|
| 1 | Conv2D + MaxPool | 32 | ReLU |
| 2 | Conv2D + MaxPool | 64 | ReLU |
| 3 | Conv2D + MaxPool | 128 | ReLU |
| 4 | Conv2D + MaxPool | 256 | ReLU |
| 5 | Dense | 512 | ReLU + Dropout (0.5) |
| 6 | Output | 6 | Softmax |

**Optimizer:** Adam (lr = 1e-4)  
**Loss Function:** Categorical Crossentropy  
**Metrics:** Accuracy  
**Callbacks:** EarlyStopping (patience=10) + ReduceLROnPlateau  

---

## ðŸ“Š Training Results

| Metric | Training | Validation |
|---------|-----------|------------|
| Accuracy | ~96% | ~93% |
| Loss | steadily decreases | low overfitting |

- Training graph is saved as:  
  `report/training_curve.png`  
- Model is saved automatically to:  
  `model/sign_language_cnn_model_full.h5`

---

## ðŸŽ¥ Real-Time Detection

The `realtime_predict.py` script integrates **MediaPipe Hands** and **OpenCV** for gesture recognition directly from the webcam.

### âœ¨ Features
- Real-time hand tracking and bounding box visualization  
- Smooth prediction history (averaged for stability)  
- Confidence thresholding (â‰¥60%)  
- Full-screen webcam mode  
- Press **q** or **Esc** to exit  

---

## ðŸ’» Technologies Used

- **Language:** Python 3.12.6  
- **Frameworks:** TensorFlow, Keras  
- **Libraries:**  
  - OpenCV  
  - MediaPipe  
  - NumPy  
  - Matplotlib  
  - SciPy  
  - Pillow  

---

## âš™ï¸ Installation & Project Run Process

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/arsh-mohan6/Sign-Language-Detection.git
cd Sign-Language-Detection
```

### 2ï¸âƒ£ Create a Virtual Environment

```bash
python -m venv venv
venv\Scripts\activate       # (Windows)
# or
source venv/bin/activate    # (Mac/Linux)
```

### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Train the Model

```bash
cd notebooks
python train_cnn.py
```

After training:
- Model â†’ `../model/sign_language_cnn_model_full.h5`  
- Training plot â†’ `../report/training_curve.png`

### 5ï¸âƒ£ Run Real-Time Gesture Detection

```bash
python realtime_predict.py
```

âœ… Opens webcam in **full screen**  
âœ… Shows bounding box, label & confidence  
âœ… Press **q** or **Esc** to close  

---

## âœ‹ Supported Gestures

| Gesture | Meaning |
|----------|----------|
| âœ‹ | Hello |
| ðŸ¤š | Bye |
| ðŸ‘ | Yes |
| ðŸ‘Ž | No |
| ðŸ¤ž | Perfect |
| ðŸ™ | Thank You |

---

## ðŸ§‘â€ðŸ’» Author

**Arsh Mohan Nishant**  
B.Tech 3rd Year | Data Science Enthusiast  

- ðŸ“§ Email: **arshmohan789@gmail.com**  
- ðŸŒ GitHub: [arsh-mohan6](https://github.com/arsh-mohan6)  
- ðŸ’¼ LinkedIn: [Arsh Mohan Nishant](https://www.linkedin.com/in/arsh-mohan-nishant-6704222a9)

---

## ðŸš€ Future Improvements

- Add more gestures and dynamic sequences  
- Deploy model using Streamlit or Flask  
- Convert to TensorFlow Lite for mobile  
- Add Text-to-Speech for recognized signs  

---

> ðŸ’¬ *"AI isnâ€™t about replacing communication â€” itâ€™s about enabling it for everyone."*
